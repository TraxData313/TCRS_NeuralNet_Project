{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from networkMod import Network\n",
    "import matplotlib.patches as mpatches\n",
    "from utils import println\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# - Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# - Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# - Taka care of the dummy variable trap:\n",
    "X = X[:, 1:]\n",
    "\n",
    "# - Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# - Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a network...\n",
      "Done!\n",
      "R: 11 -> 5x20 -> 2\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# - Initiate the classifier:\n",
    "input_size = len(X_test[0])\n",
    "hidden_count = 5\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "stupid_reward = 0 # use weighted reward by default\n",
    "print()\n",
    "print(\"Creating a network...\")\n",
    "sampleNetwork = Network(input_size,\n",
    "                        hidden_size,\n",
    "                        output_size,\n",
    "                        hidden_count)\n",
    "time.sleep(0.5)\n",
    "print(\"Done!\")\n",
    "print(sampleNetwork)\n",
    "\n",
    "\n",
    "# - Prepare the grapth data:\n",
    "accuracy = []\n",
    "step = []\n",
    "Fps  = []\n",
    "Fns  = []\n",
    "learning_rate = []\n",
    "learning_rate.append(accuracy)\n",
    "learning_rate.append(step)\n",
    "learning_rate.append(Fps)\n",
    "learning_rate.append(Fns)\n",
    "\n",
    "\n",
    "# - Traing the model:\n",
    "println(5)\n",
    "epochs = 100\n",
    "for epoch_numb in range(epochs):\n",
    "    y_pred = []\n",
    "    score_list = [0]*100\n",
    "    for test_numb in range(len(X_train)):\n",
    "        # - Get input:\n",
    "        X_train[test_numb]\n",
    "        input_ = X_train[test_numb]\n",
    "        \n",
    "        # - Input to the neural net:\n",
    "        sampleNetwork.getInputAndPropagate(input_)\n",
    "        \n",
    "        # - Get the output:\n",
    "        output = sampleNetwork.returnOutputPlace()\n",
    "        y_pred.append(output)\n",
    "        \n",
    "        # - Compare with the y_train output:\n",
    "        if stupid_reward == 1:\n",
    "            if y_train[test_numb] == output:\n",
    "                feedback =  0.5\n",
    "            else:\n",
    "                feedback = -0.5\n",
    "        elif stupid_reward == 0:\n",
    "            if y_train[test_numb] == 1:\n",
    "                # Max punishment for false positives (max=0.5)\n",
    "                if output == 1:\n",
    "                    feedback = 0.5\n",
    "                else:\n",
    "                    feedback = -0.5\n",
    "            if y_train[test_numb] == 0:\n",
    "                # Adjust smart reward here:\n",
    "                if output == 0:\n",
    "                    feedback = 0.17\n",
    "                else:\n",
    "                    feedback = -0.17\n",
    "        \n",
    "        # - Feedback(Reward):\n",
    "        sampleNetwork.rewardAndUpdate(feedback)\n",
    "\n",
    "        # - Calculate score (% of right answers):\n",
    "        if feedback > 0:\n",
    "            new_score = 1\n",
    "        else:\n",
    "            new_score = 0\n",
    "        score_list.append(new_score)\n",
    "        score_list.pop(0)\n",
    "\n",
    "        # - Every Nth turn:\n",
    "        Nth_turn = 1000\n",
    "        if test_numb % Nth_turn == 0 and test_numb != 0 and test_numb != Nth_turn:\n",
    "        \n",
    "            # Conf matrix:\n",
    "            cm = confusion_matrix(y_train[:len(y_pred)], y_pred)\n",
    "            total_error = (cm[1][0]+cm[0][1]) / np.sum(cm)\n",
    "            false_positives = cm[1][0]/ (cm[1][0]+cm[1][1])\n",
    "            false_negatives = cm[0][1]/ (cm[0][0]+cm[0][1])\n",
    "        \n",
    "            correctness = np.mean(score_list)\n",
    "        \n",
    "            # - Append the learning rate list:\n",
    "            learning_rate[0].append(total_error)\n",
    "            try:\n",
    "                learning_rate[1].append(np.mean(learning_rate[1][-1]+Nth_turn))\n",
    "            except:\n",
    "                learning_rate[1].append(0)\n",
    "            learning_rate[2].append(false_positives)\n",
    "            learning_rate[3].append(false_negatives)\n",
    "                \n",
    "            # - Print data:\n",
    "            print()\n",
    "            print(\"Epoch  :\", epoch_numb)\n",
    "            print(\"Test   :\", test_numb)\n",
    "            print(\"Error %:\", total_error*100)\n",
    "            print(\"- False positives %:\", false_positives*100)\n",
    "            print(\"- False negatives %:\", false_negatives*100)\n",
    "            \n",
    "            plt.xlabel('Training sample')\n",
    "            plt.ylabel('Correctness')\n",
    "            plt.plot(learning_rate[1], learning_rate[0], color=\"green\")\n",
    "            plt.plot(learning_rate[1], learning_rate[2], color=\"red\")\n",
    "            plt.plot(learning_rate[1], learning_rate[3], color=\"blue\")\n",
    "            green_line = mpatches.Patch(color='green', label='Total Error')\n",
    "            red_line = mpatches.Patch(color='red', label='False Positives')\n",
    "            blue_line = mpatches.Patch(color='blue', label='False Negatives')\n",
    "            plt.legend(handles=[green_line, red_line, blue_line])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Training sample')\n",
    "plt.ylabel('Correctness')\n",
    "plt.plot(learning_rate[1], learning_rate[0], color=\"green\")\n",
    "plt.plot(learning_rate[1], learning_rate[2], color=\"red\")\n",
    "plt.plot(learning_rate[1], learning_rate[3], color=\"blue\")\n",
    "green_line = mpatches.Patch(color='green', label='Total Error')\n",
    "red_line = mpatches.Patch(color='red', label='False Positives')\n",
    "blue_line = mpatches.Patch(color='blue', label='False Negatives')\n",
    "plt.legend(handles=[green_line, red_line, blue_line])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "score_list = [0]*100\n",
    "for test_numb in range(len(X_test)):\n",
    "    # - Get input:\n",
    "    X_test[test_numb]\n",
    "    input_ = X_test[test_numb]\n",
    "    \n",
    "    # - Input to the neural net:\n",
    "    sampleNetwork.getInputAndPropagate(input_)\n",
    "    \n",
    "    # - Get the output:\n",
    "    output = sampleNetwork.returnOutputPlace()\n",
    "    y_pred.append(output)\n",
    "    \n",
    "    # - Compare with the y_train output:\n",
    "    if y_test[test_numb] == output:\n",
    "        feedback = 0.5\n",
    "    else:\n",
    "        feedback = -0.5\n",
    "    \n",
    "    # - Continue learning through the test set?:\n",
    "    #sampleNetwork.rewardAndUpdate(feedback)\n",
    "    \n",
    "    # - Calculate score (% of right answers):\n",
    "    if feedback == 0.5:\n",
    "        new_score = 1\n",
    "    else:\n",
    "        new_score = 0\n",
    "    score_list.append(new_score)\n",
    "    score_list.pop(0)\n",
    "    \n",
    "    # - print every 100 turns:\n",
    "    if test_numb % 100 == 0:\n",
    "        correctness = np.mean(score_list)\n",
    "        print(test_numb)\n",
    "\n",
    "print(\"Tested network:\")\n",
    "print(sampleNetwork)\n",
    "print(\"- Trained for\", epochs, \"epochs\")\n",
    "print()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "print(\"Correct answers in %:\", ((cm[0][0]+cm[1][1])/2000.)*100)\n",
    "print(\"-  False negatives %:\", (cm[0][1] / (cm[0][1]+cm[0][0]))*100)\n",
    "print(\"-  False positives %:\", (cm[1][0] / (cm[1][1]+cm[1][0]))*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
